{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"PySpark vs Spark SQL\"\n",
        "format: \n",
        "  html:\n",
        "    toc: false\n",
        "    code-tools: true  \n",
        "    code-copy: true\n",
        "    code-fold: false\n",
        "    self-contained: true\n",
        "    anchor-sections: false\n",
        "    theme: cerulean\n",
        "---"
      ],
      "id": "07e2868a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or maybe, pyspark.sql vs. spark.sql?\n",
        "\n",
        "What are the difference between these two? \n",
        "\n",
        "[Official Comprehensive Documentation](https://spark.apache.org/docs/latest/)\n",
        "[Official PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)\n",
        "[Interesting DataBricks Documentation Link](https://databricks.com/databricks-documentation)\n",
        "\n",
        "\n",
        "<!-- Here starts the section of the comparisons of the two in column format -->\n",
        "\n",
        "\n",
        ":::::::::::::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "<p align=\"center\">__PySpark__</p>\n",
        "\n",
        "PySpark has the SQL Functions module often imported as `F.function()`. It seems that is where most of the magic is with PySpark. \n",
        "\n",
        "\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "<p align=\"center\">__Spark SQL__</p>\n",
        "\n",
        "\n",
        "I believe that spark.sql is a SQL port into ANSI SQL. Spark SQL brings SQL to Spark, and this is brought to Python? \n",
        "\n",
        "This has all the things that SQL can do. PySpark has their own versions of SQL things. \n",
        "\n",
        ":::\n",
        "::::::::::::::\n",
        "\n",
        "\n",
        "<h4 align=\"center\">Very Useful Links</h4>\n",
        "\n",
        ":::::::::::::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "\n",
        "- [PySpark Overview w/ API Reference at bottom](https://sparkbyexamples.com/pyspark-tutorial/)\n",
        "- [PySpark Tutorials](https://sparkbyexamples.com/category/pyspark/)\n",
        "\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "\n",
        "\n",
        "- [SparkbyExample SQL Functions](https://sparkbyexamples.com/spark/spark-sql-functions/)\n",
        "- [Read the Docs-esk style from Apache.org SQL Functions](https://sparkbyexamples.com/spark/spark-sql-functions/)\n",
        "\n",
        ":::\n",
        "::::::::::::::\n",
        "\n",
        "\n",
        "\n",
        "### Window Functions\n",
        "\n",
        "\n",
        "The dplyr equivalent of a window function is a group_by() %>% mutate(). This keeps all the values where group_by() %>% summarize() does not. \n",
        "\n",
        "Partioning and grouping is similar, but different. Instead of gathering data into groups and then summarizing a value by groups and shorten this down into rows unique rows.\n",
        "\n",
        "Would partioning work for making a rolling window?\n",
        "\n",
        "The code to do it in PySpark is "
      ],
      "id": "326173be"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "windowval = (Window.partitionBy('class').orderBy('time')\n",
        "             .rowsBetween(Window.unboundedPreceding, 0))\n",
        "df_w_cumsum = df.withColumn('cum_sum', F.sum('value').over(windowval))\n",
        "df_w_cumsum.show()\n"
      ],
      "id": "d6dc18c4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MLlib\n",
        "\n",
        "\n",
        "Now, the Spark ML library only works with numeric data. But we still want to use the Sex and the Embarked column. For that, we will need to encode them. To do it letâ€™s use something called the StringIndexer:\n"
      ],
      "id": "0e380103"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "dataset = StringIndexer(\n",
        "    inputCol='Sex', \n",
        "    outputCol='Gender', \n",
        "    handleInvalid='keep').fit(dataset).transform(dataset)\n",
        "\n",
        "dataset = StringIndexer(\n",
        "    inputCol='Embarked', \n",
        "    outputCol='Boarded', \n",
        "    handleInvalid='keep').fit(dataset).transform(dataset)\n",
        "\n",
        "dataset.show()"
      ],
      "id": "59a64091",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}